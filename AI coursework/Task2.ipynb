{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Value Iteration ===\n",
      "Value at (1,1): 0\n",
      "Policy at (1,1): up\n",
      "\n",
      "=== Q-Learning ===\n",
      "Q for (1,1): {'up': -10.157689560370985, 'down': -7.24905528249796, 'left': -10.011283110739981, 'right': -7.250514297359016}\n",
      "Policy for (1,1): down\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "###############################################################################\n",
    "# 1) Maze Definition and Padding\n",
    "###############################################################################\n",
    "world = [\n",
    "    \"wwwwwwwwwwwwwwwwwwwww\",\n",
    "    \"wa         o      o w\",\n",
    "    \"w www www wwwww www w\",\n",
    "    \"w o                w\",\n",
    "    \"w o www ooo ooo www w\",\n",
    "    \"w o   o ooo ooo o   w\",\n",
    "    \"w www wwwww wwwww www\",\n",
    "    \"w      o     o      w\",\n",
    "    \"www www ooo ooo wwwww\",\n",
    "    \"w   o ooooooo o o   w\",\n",
    "    \"w www ooooooo o www w\",\n",
    "    \"w    o   ooo   o    w\",\n",
    "    \"w wwwww www wwwww www\",\n",
    "    \"w o     o o     o   w\",\n",
    "    \"w o wwwww wwwww o www\",\n",
    "    \"w o               o w\",\n",
    "    \"w www www www www www\",\n",
    "    \"w      o   g   o    w\",\n",
    "    \"wwwwwwwwwwwwwwwwwwwww\"\n",
    "]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Rewards Definition\n",
    "###############################################################################\n",
    "rewards = {\n",
    "    'w': -5,  \n",
    "    'o': -20, \n",
    "    'g': 80,\n",
    "    ' ': -1,\n",
    "    'a': -1\n",
    "}\n",
    "\n",
    "max_len = max(len(row) for row in world)\n",
    "maze = [list(row.ljust(max_len)) for row in world]\n",
    "\n",
    "###############################################################################\n",
    "# 3) GridWorld Environment (Dictionary-based approach)\n",
    "###############################################################################\n",
    "class GridWorld:\n",
    "    def __init__(self, grid, rewards):\n",
    "        self.grid = np.array(grid)\n",
    "        self.rewards = rewards\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.done = False\n",
    "        \n",
    "        # Find 'g' (goal) and 'a' (agent start)\n",
    "        self.goal = None\n",
    "        self.start = None\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                if self.grid[i, j] == 'g':\n",
    "                    self.goal = (i, j)\n",
    "                if self.grid[i, j] == 'a':\n",
    "                    self.start = (i, j)\n",
    "        if self.start is None:\n",
    "            self.start = (0, 0)\n",
    "        \n",
    "        self.state = self.start\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Actions: 'up', 'down', 'left', 'right'\n",
    "        Returns (next_state, reward, done).\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self.state, 0, True\n",
    "        \n",
    "        i, j = self.state\n",
    "        if action == 'up':\n",
    "            i -= 1\n",
    "        elif action == 'down':\n",
    "            i += 1\n",
    "        elif action == 'left':\n",
    "            j -= 1\n",
    "        elif action == 'right':\n",
    "            j += 1\n",
    "        \n",
    "        # Out of bounds => treat as wall\n",
    "        if i < 0 or i >= self.grid.shape[0] or j < 0 or j >= self.grid.shape[1]:\n",
    "            return self.state, self.rewards['w'], True\n",
    "        \n",
    "        # If cell is a wall => penalty, episode ends\n",
    "        if self.grid[i, j] == 'w':\n",
    "            return self.state, self.rewards['w'], True\n",
    "        \n",
    "        self.state = (i, j)\n",
    "        \n",
    "        # If reached the goal => big reward, done\n",
    "        if self.state == self.goal:\n",
    "            self.done = True\n",
    "            return self.state, self.rewards['g'], True\n",
    "        \n",
    "        reward = self.rewards.get(self.grid[i, j], -1)\n",
    "        return self.state, reward, False\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\"All valid states except walls.\"\"\"\n",
    "        states = []\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                if self.grid[i, j] != 'w':\n",
    "                    states.append((i, j))\n",
    "        return states\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\"No actions if it's the goal; otherwise up/down/left/right.\"\"\"\n",
    "        if state == self.goal:\n",
    "            return []\n",
    "        return self.actions\n",
    "\n",
    "###############################################################################\n",
    "# 4) Value Iteration (Dictionary-based V)\n",
    "###############################################################################\n",
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iter=1000):\n",
    "    states = env.get_all_states()\n",
    "    V = {s: 0.0 for s in states}\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            old_val = V[s]\n",
    "            best_val = float('-inf')\n",
    "            acts = env.get_possible_actions(s)\n",
    "            if not acts:\n",
    "                continue\n",
    "            for a in acts:\n",
    "                saved_state = env.state\n",
    "                env.state = s\n",
    "                s_next, reward, done = env.step(a)\n",
    "                env.state = saved_state\n",
    "                \n",
    "                candidate = reward + (0 if done else gamma * V[s_next])\n",
    "                best_val = max(best_val, candidate)\n",
    "            V[s] = best_val\n",
    "            delta = max(delta, abs(old_val - best_val))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        actions = env.get_possible_actions(s)\n",
    "        if not actions:\n",
    "            policy[s] = None\n",
    "            continue\n",
    "        best_a = None\n",
    "        best_val = float('-inf')\n",
    "        for a in actions:\n",
    "            saved_state = env.state\n",
    "            env.state = s\n",
    "            s_next, reward, done = env.step(a)\n",
    "            env.state = saved_state\n",
    "            \n",
    "            candidate = reward + (0 if done else gamma * V[s_next])\n",
    "            if candidate > best_val:\n",
    "                best_val = candidate\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "###############################################################################\n",
    "# 5) Q-Learning (Dictionary-based)\n",
    "###############################################################################\n",
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1, max_steps=1000):\n",
    "    states = env.get_all_states()\n",
    "    Q = {s: {a: 0.0 for a in env.get_possible_actions(s)} for s in states}\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        s = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            actions = env.get_possible_actions(s)\n",
    "            if not actions:\n",
    "                break\n",
    "            if random.random() < epsilon:\n",
    "                a = random.choice(actions)\n",
    "            else:\n",
    "                a = max(Q[s], key=Q[s].get)\n",
    "            \n",
    "            s_next, reward, done = env.step(a)\n",
    "            \n",
    "            if s_next in Q and env.get_possible_actions(s_next):\n",
    "                max_q_next = max(Q[s_next].values())\n",
    "            else:\n",
    "                max_q_next = 0.0\n",
    "            \n",
    "            old_q = Q[s][a]\n",
    "            Q[s][a] = old_q + alpha * (reward + gamma * max_q_next - old_q)\n",
    "            \n",
    "            s = s_next\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    policy = {}\n",
    "    for s, actions_dict in Q.items():\n",
    "        if len(actions_dict) == 0:\n",
    "            policy[s] = None\n",
    "        else:\n",
    "            policy[s] = max(actions_dict, key=actions_dict.get)\n",
    "    return Q, policy\n",
    "\n",
    "###############################################################################\n",
    "# 6) Example Usage\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld(maze, rewards)\n",
    "    \n",
    "    V_vi, policy_vi = value_iteration(env, gamma=0.9, theta=1e-6, max_iter=1000)\n",
    "    print(\"=== Value Iteration ===\")\n",
    "    print(\"Value at (1,1):\", V_vi.get((1,1), None))\n",
    "    print(\"Policy at (1,1):\", policy_vi.get((1,1), None))\n",
    "    \n",
    "    Q, policy_q = q_learning(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1, max_steps=1000)\n",
    "    print(\"\\n=== Q-Learning ===\")\n",
    "    print(\"Q for (1,1):\", Q.get((1,1), None))\n",
    "    print(\"Policy for (1,1):\", policy_q.get((1,1), None))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
